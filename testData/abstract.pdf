    abstract 
    The k-nearest neighbor (KNN) rule is a classical and yet very effective nonparametric technique in pat- tern classification, but its classification performance severely relies on the outliers.The local mean-based k-nearest neighbor classifier (LMKNN) was firstly introduced to achieve robustness against outliers by computing the local mean vector of k nearest neighbors for each class.However, its performances suffer from the choice of the single value of k for each class and the uniform value of k for different classes.In this paper, we propose a new KNN-based classifier, called multi-local means-based k-harmonic nearest neighbor (MLM-KHNN) rule. In our method, the k nearest neighbors in each class are first found, and then used to compute k different local mean vectors, which are employed to compute their harmonic mean distance to the query sample.Finally, MLM-KHNN proceeds in classifying the query sample to the class with the minimum harmonic mean distance. The experimental results, based on twenty real-world datasets from UCI and KEEL repository, demonstrated that the proposed MLM-KHNN classifier achieves lower classification error rate and is less sensitive to the parameter k, when compared to nine related competitive KNN-based classifiers, especially in small training sample size situations.
    1、	Introduction
    The k-nearest neighbor (KNN) rule (Cover, & Hart, 1967) is one of the most famous classification techniques due to its simplicity, effectiveness and intuitiveness (Jiang, Pang, Wu, & Kuang, 2012; Xia, Mita, & Shibata, 2016; Xu et al., 2013). It assigns the class label to the query sample that appears most frequently in its k nearest neighbors through a majority vote. The KNN classifier has been widely studied and extensively applied in practice (Rodger, 2014&2015), thanks to its several attractive properties. In fact, the KNN rule works as a nonparametric technique, which does not require a priori knowledge about the probability distribution of the classification problem (Li, Chen, & Chen, 2008). Such property is particularly important, especially for cases where the Gaussian distributions of the samples are difficult to assume, as for small training sample size situations (Zhang, Yang, & Qian, 2012). Addi- tionally, the classification performance of the KNN rule only relies on the method of the used distance metric and one parameter k (Garcia-Pedrajas, Del-Castillo, & Cerruela-Garcia, 2015; Hu, Yu, & Xie, 2008), which represents the neighborhood size of the query sample. Finally, it has been proven that the KNN rule can asymptotically approach the optimal classification performance achieved by the Bayes method under the constraint of k/N → 0 (Wagner, 1971), where N is the total number of training samples. 
    While the KNN rule has many significant advantages, some problems still exist. The first problem is that all the k nearest neighbors are equally considered when assigning a class label to the query sample via a simple majority vote. Obviously, this is not reasonable when the k nearest neighbors are very different in their distances to the query sample and some closer nearest neighbors seem to be more important (Bailey, & Jain, 2010). In or- der to tackle this problem, several distance-weighted voting meth- ods (Mateos-García, García-Gutiérrez, & Riquelme-Santos, 2016) for KNN rule have been developed, where larger weights are given to the closer nearest neighbors. However, such approach is not always correct, as some farther neighbors may be more impor- tant for classification. Accordingly, a number of adaptive metric nearest neighbor classifications were developed (Yang, Wei, & Tao, 2013; Weinberger, & Saul, 2009). The second problem is that the KNN rule cannot properly classify the query sample when attribute data are similar to the training samples from different classes (Liu, Pan, & Dezert, 2013). In fact, the number of the nearest neighbors from different classes may be similar in the k-nearest neighbors of the query, hence causing the KNN rule to incorrectly assign a class label. Accordingly, a number of fuzzy classifications and the belief-based classifications have been derived in order to allow the queries to belong to different classes with masses of belief, hence reducing the classification errors (Liu, Pan, Dezert, & Mercier, 2014; Sarkar, 2007). The third problem is that the classification perfor- mance of the KNN rule severely relies on the distance function used to compute the distance between query sample and train- ing sample. As a result, several global and local feature weighting- based distance metric learning methods (Chen, & Gou, 2015; Lin, Li, Lin, & Chen, 2014) were proposed to improve the performance of the KNN classifier. 
    It is also known that nonparametric classifiers suffer from out- liers, especially in the situations of small training sample size (Mitani, & Hamamoto, 2006; Zhang et al., 2012). That is one reason why the classification performance of KNN rule is heavily influ- enced by the neighborhood size k (Bhattacharya, Ghosh, & Chowd- hury, 2015; Wang, Neskovic, & Cooper, 2006). In fact, if k is very small, the classification decision can be poor due to some noisy and imprecise samples (Liu et al., 2013). On the contrary, a large value of k can lead to degradation in the classification performance because of the existing outliers in the k nearest neighbors that come from the wrong classes. In order to design a practical clas- sifier that is more robust to outliers, a simple nonparametric clas- sifier named local mean-based k-nearest neighbor (LMKNN) classi- fier was proposed in Mitani, & Hamamoto, 2006). Because of the effectiveness and easy design of the LMKNN classifier, its core idea has been successfully applied to many other improved methods (Gou et al., 2014; Sansudin, & Bradley, 2010; Zeng, Yang, & Zhao, 2009a, 2009b). 
    Even though the LMKNN classifier can be easily designed in practice with a good classification performance, it generally suf- fers from the choice of the single value of k for each class and the uniform value of k for different classes. Thus, in order to design a classifier with better classification performance and less sensi- tivity to the neighborhood size k, we propose a new KNN-based classifier, which is called the multi-local means-based k-harmonic nearest neighbor (MLM-KHNN) classifier. The MLM-KHNN classifier mainly holds two key properties when compared with the LMKNN classifier: 
    (1)  The MLM-KHNN classifier employs as many as k multi-local means for each class instead of a single local mean, as for the LMKNN classifier, to reduce the sensitivity to the choice of neighborhood size k. 

    (2)  The MLM-KHNN classifier for the first time introduces the harmonic mean distance as the similarity measure, hence re- ducing the error rate by focusing on the more reliable local means in different classes instead of a uniform value of k in all classes, as in the LMKNN classifier. 

    The rest of paper is organized as follows. In Section 2, we briefly describe the state-of-the-art k-nearest neighbor classifiers, focusing on the approaches based on the LMKNN rule. The preliminaries of both the LMKNN rule and the harmonic mean distance for the classification problems are given in Section 3. In Section 4, we propose our MLM-KHNN classifier. Extensive experiments to compare the proposed approach with other competitive KNN-based classifiers on UCI (Merz & Murphy, 1996) and KEEL (Alcalá-Fdez, Fernández, Luengo, Derrac, & García, 2011) real-world datasets are conducted in Section 5. Finally, discussions and conclusions are given in Section 6 and Section 7, respectively. 
    2、Related work 
    In this section, we briefly review some related state-of-the-art KNN-based classifiers, with a particular focus on the LMKNN classifier (Mitani, & Hamamoto, 2006) and its extensions. 
    The traditional KNN classifier is a simple and powerful technique in pattern classification, but one major problem is that its performance severely relies on outliers. The LMKNN classifier was firstly introduced to tackle this problem and enhance the classification performance. In the LMKNN rule, the local mean vector is firstly computed according to all the k nearest neighbors in each class, and then the query sample is assigned to the class with the minimum Euclidean distance between the query and local mean vector. However, the LMKNN classifier selects the single value of k for each class and the uniform value of k for all classes, which may lead to misclassification. 
    To further improve the classification performance of the LMKNN classifier, several different local mean-based k-nearest neighbor classifiers have been proposed. The pseudo k-nearest neighbor classifier (PNN) (Zeng, Yang, & Zhao, 2009a, 2009b) was developed to address the problem caused by the choice of a single value of k. In the PNN classifier, the sum of the weighted distances be- tween the query sample and its k nearest neighbors in each class is used as the similarity measure between the query and the pseudo training sample. Based on the basic ideas of both the LMKNN classifier and PNN classifier, the local mean-based pseudo k-nearest neighbor classifier (LMPNN) was proposed in Gou et al. (2014) and showed promising classification performance. In the LMPNN classifier, the pseudo distance is obtained by combining the weighted distance between the query sample and each local mean vector in different classes. Additionally, some other LMKNN-based classifiers were also successfully applied to different areas, such as the local mean and class mean-based k-nearest neighbor classifier (Zeng, Yang, & Zhao, 2010) and the nearest neighbor group-based classifier (Sansudin, & Bradley, 2010). 
    Additional approaches were also developed in order to address the problems caused by outliers. The mutual k-nearest neighbor classifier (MKNN) (Liu, & Zhang, 2012) represents a simple and powerful technique to find the more reliable nearest neighbors through noisy data elimination procedure. In the MKNN rule, only the training sample that also regards the query as one of its k nearest neighbors will be selected as the mutual nearest neigh- bor. The coarse to fine k-nearest neighbor classifier (CFKNN) (Xu et al., 2013) is another successful method for selecting the nearest neighbors from the view of optimally representing the query sam- ple. It first coarsely selects the close training samples in terms of a small number, and then finely determines the k nearest neigh- bors of the query sample. Through the coarse-to-fine procedure, the CFKNN classifier can accurately classify with less redundant in- formation and outliers in the obtained training dataset. 
    Another popular research direction of the KNN rule resides in the study of fuzzy k-nearest neighbor-based algorithms, which led to several works (Keller, Gray, & Givens, 1985; Liu, Pan, Dezert, & Mercier, 2014; Rodger, 2014; Sarkar, 2007) that exploit the fuzzy uncertainty to enhance the classification result of the KNN rule. Among them, the fuzzy-rough nearest neighbor classifier (FRNN) (Sarkar, 2007) showed to be able to obtain richer class confidence values with promising classification results without the need to know the optimal value of k. Moreover, a hybrid k-nearest neigh- bor classifier (HBKNN) was proposed in Yu, Chen, Liu, & You (2016), proving the suppleness and effectiveness of combining the fuzzy membership in the fuzzy KNN classifier and the local information in the LMKNN classifier. 
    In this paper, in order to design a local mean-based near- est neighbor classifier with better classification performance and less sensitivity to the neighborhood size k, we propose a new KNN-based classifier, which is called multi-local means-based k- harmonic nearest neighbor (MLM-KHNN) rule. Instead of employ- ing only one local mean vector based on a fixed single value of k in each class as in the LMKNN rule, we improve it by computing all k different local mean vectors based on the k nearest neighbors in each class. Additionally, as the local sample distribution in each class is different, the value of k to obtain the nearest local mean vector is not always the same for all classes. To take into account the importance of the nearer local mean vector to the query sam- ple under different values of k in each class, the harmonic mean distance of the k multi-local mean vectors to the query sample is introduced for the first time. 
    3、Preliminaries 
    In this section, we briefly describe the LMKNN classifier and the harmonic mean distance for classification problems. 
    3、1、The LMKNN classifier 
    The local mean-based k-nearest neighbor (LMKNN) rule (Mitani, & Hamamoto, 2006) is a simple, effective and robust nonparamet- ric classifier. Mitani and Hamamoto have demonstrated that it can improve the classification performance and also reduce the influ- ence of existing outliers, especially in small training sample size situations. 
    Given a training sample set source with Ntr training samples yi ∈ RD in a d -dimensional feature space from M classes, ci is the corresponding class label of yi, where ci ∈ {ω1, ω2, ⋅⋅⋅, ωM  }. Let source denote the training sample set of class ωj, where Nj is the number of the training samples in class ωj and source. Instead of finding the original k nearest neighbors in the whole training dataset Tr as in the KNN rule, the LMKNN rule is designed to employ the local mean vector of k nearest neighbors for each class of Trj to classify the query sample. In the LMKNN rule, a given query sample x ∈ RD is classified into class ωc by the following steps:
    Step 1、 Find the k nearest neighbors of x from the Trj of each class ωj . Let source denote the set of the k nearest neighbors of the query sample x in class ωj , where source is computed from Trj for each class and arranged in an ascending order according to the Euclidean distance measure.
    Step 2、 Compute the local mean vector source of class ωj by using the k nearest neighbors in the set.
    Step 3、 Classify x into the class ωc when the Euclidean distance between its local mean vector and x is the minimum among the M classes.
    Note that the LMKNN classifier is equivalent to the 1-NN classifier when k = 1, and the meaning of parameter k is totally different from that in KNN rule. In the KNN rule, the k nearest neighbors are chosen from the whole training dataset Tr, while the LMKNN rule employs the local mean vector of k nearest neighbors in each class Trj. Instead of the majority vote in k nearest neighbors, the LMKNN rule aims at finding the class with the closest local region to the query sample, which can effectively overcome the negative effect of outliers by computing the local mean vector of k nearest neighbors for each class.
    3、2、 The harmonic mean distance
    The harmonic mean distance is introduced to measure the distance between a pair of point groups. We briefly describe the concept of harmonic average, in order to clarify the rationale behind its usage in the proposed method.
    Given a dataset with k elements {y1, y2, ⋅⋅⋅, yk}, its harmonic average is defined as.
    Note that if one element in y1, y2, ⋅⋅⋅, yk is small, their harmonic average can be very small. In other words, the value of HA({y1, y2, ⋅⋅⋅, yk}) relies more on the element with a smaller value in {y1, y2, ⋅⋅⋅, yk}.
    The basic idea of harmonic mean distance is to take the sum of the harmonic average of the Euclidean distances between one given data point and each data point in another point group. In this paper, we first apply the harmonic mean distance to the KNN-based classification problem. The harmonic mean distance, which is defined as HMD( · ), is used to measure the distance between a query sample x and its related training sample group. For example, given a query sample x and its k nearest neighbors set from the training sample set, the harmonic mean distance between x and the set NNk(x) can be obtained as.
    In order to highlight the differences between the arithmetic mean distance and the harmonic mean distance used in the proposed method, their comparisons are given in the following.
    The arithmetic mean distance between x and its related k nearest neighbors, which is denoted as , can be expressed as the weighted sum of  as in Eq. (5), while the value of represents the weight of to compute the final value of . As shown in Eq. (6), it can be proven that is always true, which means that the are considered equally important in the arithmetic mean distance.
    However, the value of, which represents the weight of  in computing the , is totally different, because  is inversely proportional to the value of  as shown in Eq. (7). Compared with the arithmetic mean distance, the harmonic mean distance focuses more on the influence of the sample that has a closer distance to the query sample x. Besides, it can be indicated that if in  has a small distance to x  , the value of  will be small.
    4、The proposed MLM-KHNN classifier
    In this section, we describe the proposed multi-local means-based k-harmonic nearest neighbor (MLM-KHNN) classifier. The goal of the MLM-KHNN rule is to improve the classification performance and reduce the sensitivity to the single value of neighborhood size k for each class and the uniform value of k for all classes in the LMKNN rule.
    The mechanism of the LMKNN rule is to assign the query sample the class label of its most similar local subclass from different classes, where the local mean vector is used as the representation of the local subclass. Obviously, the local subclasses are obtained based on the k nearest neighbors in each class. Thus, the choice of parameter k is of great importance to generate the local mean vector that can better represent its own class. However, there are two main problems for the choice of the parameter k in the LMKNN rule, which may lead to misclassification. Firstly, only a fixed single value of k is employed in each class, which may lead to the high sensitivity of the local mean to the value of k. If k is too small, the useful classification information may be insufficient, whereas a large value of k can easily lead to outliers to be included in the k nearest neighbors of the true class ( Gou et al., 2014). Secondly, a uniform value of k is employed in all classes. Since the local sample distributions in different classes are quite different, the value of k for selecting the most similar local subclass of the query for each class is usually very different as well. Therefore, it is unreasonable to use the same value of k for all classes, as for the LMKNN rule. From Eq. (1), we have.where ‖ · ‖ denotes the Euclidean distance. From Eq. (8), it can be seen that the difference vector of x and each sample in is considered when computing the distance of x   and the local mean vector. Taking Fig. 1(a) as an example, when and>, i = 1,2, ⋅⋅⋅, it is obvious that the classification result is very sensitive when using a fixed single value of k = 3. In this example, the query x will be correctly classified into class ω1 when k = 4, but it will get the wrong classification result when k = 3. Actually, the class ω2 only has a very few samples with quite close distances to the query sample x. A similar example is also shown in Fig. 1(b), where a large value of k can easily lead to misclassification due to the outliers existing in the k nearest neighbors in the true class of the query sample. As shown in Fig. 1(b), the outlier samples and  in class ω1 pull away its local mean vector  from x due to the unsuitable selection of the value of neighborhood size k. Thus, it can be inferred that a fixed single value of k in the LMKNN rule may limit its classification performance.
    Additionally, in terms of the diversity of the local sample distribution in different classes, the value of k to obtain the nearest local mean vector of the query sample x for each class may be quite different. However, the LMKNN classifier chooses a uniform value of k for all classes, which may result in selecting extra outliers in some classes and inadequate number of nearest neighbors in other classes. In other words, it may not always select the effective subclass to represent its own class because the uniform value of k ignores the difference of local sample distribution in different classes, hence leading to misclassification.
    In order to solve this problem, we propose a new KNN-based classifier, called multi-local means-based k-harmonic nearest neighbor (MLM-KHNN) rule. In the proposed method, k different local mean vectors based on the k nearest neighbors in each class are firstly computed. Clearly, the k multi-local mean vectors in each class have different distances to the query sample, and the nearer local mean vector is more important to represent its own class for classification. In other words, we want to focus more on the values of k that can find a closer local subclass, where the values of k in each class can be totally different. To do so, we introduce the harmonic mean distance between the group of the multi-local mean vectors in each class and the query sample x to measure their similarity and finally classify the query sample to the class with the minimum harmonic mean distance. The proposed MLM-KHNN classifier achieves lower classification error rate and is less sensitive to the neighborhood size k when compared with LMKNN and other related KNN-based classifiers.
    The MLM-KHNN classifier, as a new version of the KNN rule and LMKNN rule, is presented below. The proposed method essentially shows two significant advantages when compared with the LMKNN rule. First, the k local mean vectors based on the top r (1 ≤ r ≤ k) nearest neighbors are all computed and used after the k nearest neighbors have been found in each class, which are called as multi-local means. Second, the harmonic mean distance between the group of k multi-local mean vectors and the query sample in each class is introduced to measure similarity for a more accurate classification, which takes into the influence of different values of k for classification for different classes.
    Let be a training sample set with Ntr training samples from M classes, where yi ∈ RD and ci is the corresponding class label of yi, ci ∈ {ω1, ω2, ⋅⋅⋅, ωM}. For class ωj , suppose that denotes the training sample set of class ωj with Nj training samples. In the MLM-KHNN rule, the class label ωc of a given query sample x ∈ RD is obtained as follows.
    Step 1、 For each class ωj, find the k nearest neighbors of x denoted by from Trj, which are then sorted in an ascending order according to their Euclidean distances to x.
    Step 2、 Compute k multi-local mean vectors based on the top r (1 ≤ r ≤ k) nearest neighbors of x from Trj in each class ωj. For class ωj , let  denote the k multi-local mean vectors
    Note that and their corresponding Euclidean distances to x are denoted by .
    Step 3、 For each class ωj, compute the harmonic mean distance between x and k   multi-local mean vectorsobtained from Step 2.
    Step 4、 Assign x to the class ωc that has the minimum harmonic mean distance to x in terms of Eq、 (11).
    Note that when k = 1, only has one local mean vector which is equal to , and the harmonic mean distance of x and is computed by . Thus, the MLM-KHNN rule degrades into the LMKNN rule and the KNN rule when k = 1, and all of them have the same classification performances as the 1-NN classifier. The pseudo codes of the MLM-KHNN classifier are summarized in detail in Algorithm 1.
    5、 Experiments
    In contrast with the LMKNN classifier (Mitani, & Hamamoto, 2006) and the classical KNN classifier (Cover, & Hart, 1967), the proposed MLM-KHNN classifier does not choose the local mean vectors using a fixed single value of neighborhood size k. Instead, as many as the k multi-local mean vectors based on the k nearest neighbors in each class are employed, and the harmonic mean distance between the group of multi-local mean vectors in each class and query sample is also utilized to emphasize the importance of closer local mean vectors in classification.
    To validate the proposed method on the classification performance, we compare the MLM-KHNN classifier with the standard KNN classifier and other eight competitive KNN-based classifiers: KNN (Cover, & Hart, 1967), WKNN (Dudani, 1976), LMKNN (Mitani, & Hamamoto, 2006), PNN (Zeng et al., 2009a, 2009b ), LMPNN (Gou et al., 2014), MKNN (Liu & Zhang, 2012), CFKNN (Xu et al., 2013), FRNN (Sarkar, 2007), HBKNN (Yu et al., 2016). The comparative experiments are extensively conducted on ten UCI (Merz & Murphy, 1996) and ten KEEL (Alcalá-Fdez et al., 2011) real-world datasets in terms of error rate, which is one of the most important measures in pattern classification. In our experiments, the parameter k is optimized by the cross-validation (CV) ( Toussaint, 1974) approach for each classifier, with the same approach described in (Gou et al., 2014; Mitani, & Hamamoto, 2006; Zeng et al., 2009a, 2009b ). In order to better evaluate the sensitivity of the proposed classifier to the neighborhood size k, comparative experiments of the classification performance with varying neighborhood size k are also conducted.
    5、1、 Datasets
    In this subsection, we briefly summarize the selected datasets considered in our experiments. The twenty real-world datasets are all taken from the UCI machine-learning repository and the KEEL repository, which are Air, Balance, German, Glass, Ionosphere, Landsat, Monk-2, Optigits, Phoneme, Ring, Saheart, Spambase, Segment, Tae, Texture, Thyroid, Vehicle, Vote, Vowel and Wine, respectively. Among these twenty real-world datasets, they hold quite different characteristics in numbers of samples, attributes and classes, which are listed in Table 1. The numbers of samples of these datasets are characterized by a wide range and vary from 151 to 7400 in order to comprehensively validate the proposed method.
    Since we are interested in the classification performance of the KNN-based classifiers when considering small training sample size, the size of training set needs to be determined in advance. The same approach as in (Gou et al., 2014; Mitani, & Hamamoto, 2006; Zeng et al., 2009a, 2009b) is adopted in our experiments. For each dataset, we randomly chose a training set that contains approximately 30% of the data and the others are chosen as testing data, as shown in Table 1. The experiments are repeated 50 times for a cross-validation and the average error rate with 95% confidence interval over these 50 repetitions is reported.
    5、2、 Experiments on real-world datasets
    5、2、1、 Results of the classification performance
    As previously stated, the classification performance of the proposed MLM-KHNN classifier is compared to KNN, WKNN, LMKNN, PNN, LMPNN, MKNN, CFKNN, FRNN and HBKNN rules over twenty real-world datasets of UCI and KEEL by means of the error rate. The WKNN rule is a famous distance-weighted k-nearest neighbor classifier where larger weights are given to the neighbors with a smaller distance to the query sample. The LMKNN rule employs the local mean vector in each class to classify the query sample, as described in Section 3.1. Based on the LMKNN rule, the PNN and LMPNN rule were both successfully developed in order to obtain a better classification performance. The PNN rule firstly attempts to obtain the pseudo nearest neighbor in each class, and then assigns the label of closest pseudo nearest neighbor to the query sample, whereas the LMPNN rule integrates the ideas of both LMKNN and PNN method. The MKNN rule is based on the application of the mutual nearest neighbor to obtain more reliable nearest neighbors. The CFKNN rule uses the coarse-to-fine strategy to obtain a revised training dataset that can better represent the query sample. The FRNN rule is a famous fuzzy-based k-nearest neighbor classifier with richer class confidence values based on the fuzzy-rough ownership function. Finally, the HBKNN rule combines the fuzzy membership in the fuzzy k-nearest neighbor classifier and the similar local information in the LMKNN classifier.
    The experiments are carried out by a 50-trials holdout procedure, where for each trial, the dataset is randomly divided into the testing samples and the training samples under the size shown in Table 1. In our experiments, the value of neighborhood size k for each classifier is firstly optimized by the CV approach in the training set, and then the average error rate with 95% confidence with the optimized parameter k is used for the final classification performance evaluation.
    Table 2 collects the comparison results in terms of classification performance of each classifier by giving the average error rate under the optimized value of the neighborhood size k, and the corresponding 95% confidence interval is also listed. From the experimental results in Table 2, it can be observed that the proposed MLM-KHNN classifier outperforms all nine state-of-the-art KNN-based methods for almost all of the twenty real-world datasets. In particular, the MLM-KHNN classifier significantly reduces the error rate of the standard LMKNN rule by introducing the concepts of multi-local mean vectors and harmonic mean distance similarity, as it can focus on the more reliable local mean vectors with smaller distances to the query sample in each class. With regards to the CFKNN, FRNN and HBKNN classifiers, although they hold quite low error rate on a few datasets, the proposed MLM-KHNN classifier is still able to outperform them on most of the datasets.
    In order to further validate the classification performance of the proposed MLM-KHNN classifier under different values of parameter k, the average classification error rate of all twenty datasets of the KNN, WKNN, LMKNN, PNN, LMPNN, MKNN and CFKNN and the MLM-KHNN classifiers under the condition of k = 1–15 with a step size of 2 are listed in Table 3. Note that the average classification error rate under different choice of neighborhood size k of the FRNN and the HBKNN are not listed in Table 3. This is due to the fact that the FRNN classifier employs all training samples for classification and does not need to obtain the optimal value of k, while the HBKNN classifier empirically gives the default value of k. Thus, their error rates on each dataset are constant and are independent of the parameter k.
    From the results in Table 3, it can be found that the MLM-KHNN classifier achieves the best classification performance in all cases, especially when compared with other LMKNN-based methods. As discussed in Section 4, an inappropriate value of k can easily lead to a higher error rate due to existing outliers, however the results in Table 3 indicate that the proposed method is less sensitive to outliers and can potentially reduce the error rate independently of the neighborhood size k.
    5、2、2、 Results of the sensitivity to the neighborhood size k
    To evaluate the sensitivity of the classification performance to the parameter k, the error rates of different classifiers with varying the neighborhood size k are also compared.
    The experimental comparisons of the error rate with parameter k changing from 1 to 15 on twenty real-world datasets are shown in Fig. 2. From the results shown in Fig. 2, it can be seen that the MLM-KHNN classifier has a lower error rate than other methods with different values of k for most cases. Particularly, when the value of the neighborhood size k is relatively large, the improvements of the classification performance between the MLM-KHNN and the KNN, WKNN, PNN, LMPNN, MKNN and CFKNN can be very significant. As previously stated, the classification performances of the FRNN and the HBKNN classifiers do not rely on the choice of k, hence they are not shown in Fig、 2.
    Moreover, the curve of the error rate of the proposed MLM-KHNN method is shown to be smoother and flatter when compared with other classifiers that are based on the LMKNN rule, such as the LMKNN, PNN, LMPNN classifiers. In general, we can see that the proposed MLM-KHNN can both improve the classification performance and be less sensitive to the neighborhood size k.
    6、 Discussions
    In pattern classification, computational complexity is an important issue when designing an effective classifier for practical applications. To better explain the merits of the proposed MLM-KHNN classifier, comparisons of the computational complexities between the LMKNN and MLM-KHNN classifiers are further discussed in this section. We only focus on the complexity of online computations in the classification stage.
    Let Ntr denote the number of the training samples, Nj denote the number of training samples in class ωj, p represent the feature dimensionality, M denote the number of classes and k represent the neighborhood size.
    For the traditional LMKNN classifier, the classification stage mainly contains three steps. The first step is to search for the k   nearest neighbors in each class based on the Euclidean distance, the multiplications and sum operations are all equal to O(N1p+N2p+⋯+NMp), which can also be abbreviated to O(Ntrp  ). Additionally, the comparisons are O(N1k+N2k+⋯+NMk), which are equal to O(Ntrk). The second step is to compute the local mean vector of each class, which requires O(Mpk) sum operations. The third and final step assigns the query sample to the class with the smallest distance between its local mean and the given query, and it is characterized by O(Mp) multiplications and sum operations, whereas the class label is determined with O(M)   comparisons. Thus, the total computational complexity of the LMKNN rule is O(2Ntrp+Ntrk+Mpk+2Mp+M).
    For the proposed MLM-KHNN classifier, its classification stage consists of four steps. The first step is the same as the LMKNN rule. During its second step, the MLM-KHNN method obtains the k multi-local mean vectors. It can be easily shown from the steps in Section 4 that , and can be reused from the former computations when r varies from 1 to (r−1). Thus, the sum operations are equal to O(Mpk). At the third step, the harmonic mean distance between the query x and k multi-local mean vectors is calculated for each class, which requires O(Mpk) multiplications and O(Mpk+Mk) sum operations, as illustrated in Eq. (10). Then, in the final step, the proposed method classifies the query sample to the class with the minimum harmonic mean distance to the given query with O(M)   comparisons. Thus, the total computational complexity of the MLMKHNN rule is O(2Ntrp+Ntrk+3Mpk+Mk+M).
    From the above analysis, it can be seen that the increased computation costs of the proposed method are. Since the number of classes M and the neighborhood size k are usually much smaller than the value of the training sample size Ntr, it means that the computational differences are rather small.
    7、 Conclusions
    In this paper, we proposed a new KNN-based classifier, which is called multi-local means-based k-harmonic nearest neighbor (MLM-KHNN) classifier. The proposed method aims at improving the classification performance of the KNN-based schemes, especially the local mean-based KNN classification approaches, such as the LMKNN, PNN and LMPNN classifiers. To overcome the negative influence of the single and uniform value of k used in the local mean-based KNN classifiers, the proposed method significantly enhances the classification performance mainly from two key factors. The first improvement resides in the fact that in the MLM-KHNN rule, as many as k different local mean vectors that we call multi-local mean vectors are computed in each class to achieve a more robust classification performance with lower error rate. The second improvement resides in the fact that we first applied the harmonic mean distance to measure the similarity, which allows it to focus on the more reliable local mean vectors that have smaller distances to the query sample.
    To evaluate the classification performance of the proposed MLM-KHNN rule, nine state-of-the-art KNN-based approaches have been compared: KNN, WKNN, LMKNN, PNN, LMPNN, MKNN, CFKNN, FRNN and HBKNN classifiers. Among them, in addition to the standard KNN classifier and three local mean-based KNN classifiers, we also compared our method with five other nearest neighbor classifiers related to different research topics of the KNN rule, such as the distance weighting, noisy data elimination and fuzzy nearest neighbor classifier. Experimental results on twenty real-world datasets of the UCI and KEEL repository demonstrated that the proposed MLM-KHNN classifier achieves lower classification error rate and is less sensitive to outliers under different choices of neighborhood size k.
    Furthermore, it was shown that when compared with the traditional LMKNN rule, the increased computations required by the proposed MLM-KHNN method are only related to the number of classes M and the neighborhood size k, which are usually much smaller than the value of the training sample size Ntr. Therefore, the computational differences between the MLM-KHNN classifier and the LMKNN classifier are very small.
    Acknowledgment
    This work is supported in part by the Key Science and Technology Program of Shaanxi Province (Grant No、2016GY-097), the Science and Technology Program of Xi’an Municipality (Grant No. CXY1514-6), and the Industrial Program of Zhejiang Province (Grant No. 2016C31G4180003), the Open Project Program of the National Laboratory of Pattern Recognition (Grant No. 201407370).

